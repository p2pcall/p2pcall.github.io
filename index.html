<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Webcam 2D to 3D Depth</title>
  <style>
    html, body { margin: 0; overflow: hidden; background: #000; color: #0f0; }
    canvas { display: block; }
    #video { display: none; }
    #log {
      position: absolute; bottom: 0; left: 0; right: 0;
      background: rgba(0, 0, 0, 0.8);
      color: #0f0; font-size: 12px;
      padding: 8px; height: 100px; overflow-y: auto;
      font-family: monospace;
    }
  </style>
</head>
<body>
  <video id="video" autoplay playsinline></video>
  <canvas id="canvas3d"></canvas>
  <div id="log">[log]</div>

  <!-- Libraries -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.16.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/depth-estimation"></script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.157.0/build/three.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.157.0/examples/js/controls/OrbitControls.min.js"></script>

  <script>
    const log = (msg) => {
      const logDiv = document.getElementById('log');
      const time = new Date().toLocaleTimeString();
      logDiv.innerHTML = `[${time}] ${msg}<br>` + logDiv.innerHTML;
    };

    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas3d');

    const scene = new THREE.Scene();
    const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 100);
    const renderer = new THREE.WebGLRenderer({ canvas });
    renderer.setSize(window.innerWidth, window.innerHeight);
    camera.position.z = 2;

    const controls = new THREE.OrbitControls(camera, renderer.domElement);

    const resolution = 64;
    const geometry = new THREE.PlaneGeometry(1.6, 1.2, resolution, resolution);
    const material = new THREE.MeshBasicMaterial({ vertexColors: true, side: THREE.DoubleSide });
    const mesh = new THREE.Mesh(geometry, material);
    scene.add(mesh);

    const colors = new Float32Array(geometry.attributes.position.count * 3);
    geometry.setAttribute('color', new THREE.BufferAttribute(colors, 3));

    async function init() {
      try {
        log("Accessing webcam...");
        const stream = await navigator.mediaDevices.getUserMedia({ video: { width: 256, height: 192 } });
        video.srcObject = stream;

        log("Loading depth model...");
        const model = await depthEstimation.DepthEstimation.create({
          model: 'mediapipe_small'
        });
        log("Model loaded successfully.");

        function updateMesh(depthTensor) {
          const depthData = depthTensor.dataSync();
          const positions = geometry.attributes.position.array;
          const colors = geometry.attributes.color.array;

          for (let i = 0; i < positions.length; i += 3) {
            const idx = i / 3;
            const depth = -depthData[idx] * 0.5;

            positions[i + 2] = depth;

            const colorValue = 1.0 - depth;
            colors[i] = colorValue;
            colors[i + 1] = 0.5;
            colors[i + 2] = 1.0 - colorValue;
          }

          geometry.attributes.position.needsUpdate = true;
          geometry.attributes.color.needsUpdate = true;
        }

        async function animate() {
          requestAnimationFrame(animate);

          if (video.readyState >= 2) {
            const depth = await model.estimateDepth(video);
            updateMesh(depth);
          }

          controls.update();
          renderer.render(scene, camera);
        }

        animate();

        window.addEventListener('resize', () => {
          camera.aspect = window.innerWidth / window.innerHeight;
          camera.updateProjectionMatrix();
          renderer.setSize(window.innerWidth, window.innerHeight);
        });

      } catch (err) {
        log("Error: " + err.message);
        console.error(err);
      }
    }

    init();
  </script>
</body>
</html>
